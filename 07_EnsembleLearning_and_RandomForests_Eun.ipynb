{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Seminar\n",
    "### Chapter07. Ensemble Learning and Random Forests\n",
    "<br>\n",
    "19.09.05  \n",
    "\n",
    "\n",
    "JaeEun Yoo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Boosting (originally called hypothesis boosting) refers to any Ensemble method that can **combine several weak learners into a strong learner.** \n",
    "* The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. \n",
    "* There are many boosting methods available, but by far the most popular are **AdaBoost** (short for Adaptive Boosting) and **Gradient Boosting**. Let’s start with AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost (Adaptive Boost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One way for **a new predictor to correct its predecessor** is to pay a bit more attention to the training instances that the predecessor underfitted. \n",
    "* This results in new predictors focusing more and more on the hard cases. \n",
    "* This is the technique used by AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/07_01.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For example, to build an AdaBoost classifier, a first base classifier (such as a Decision Tree) is trained and used to make predictions on the training set. \n",
    "* The relative weight of misclassified training instances is then increased. \n",
    "* A second classifier is trained using the updated weights and again it makes predictions on the training set, weights are updated, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/07_02.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Figure 7-8 shows the decision boundaries of five consecutive predictors on the moons dataset (in this example, each predictor is a highly regularized SVM classifier with an RBF kernel14). \n",
    "* The first classifier gets many instances wrong, so their **weights get boosted.** \n",
    "* The second classifier therefore does a better job on these instances, and so on. \n",
    "* The plot on the right represents the same sequence of predictors except that the learning rate is halved \n",
    "    * (i.e., the misclassified instance weights are boosted half as much at every iteration). \n",
    "* As you can see, this sequential learning technique has some similarities with **Gradient Descent**, except that instead of tweaking a single predictor’s parameters to minimize a cost function, AdaBoost **adds predictors to the ensemble**, gradually making it better.\n",
    "* Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that **predictors have different weights depending on their overall accuracy on the weighted training set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> WARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one important drawback to this sequential learning technique: **it cannot be parallelized (or only partially),** since each predictor can only be trained after the previous predictor has been trained and evaluated.  \n",
    "As a result, **it does not scale as well as bagging or pasting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let’s take a closer look at the **AdaBoost algorithm.**  \n",
    "* **Each instance weight w<sup>(i)</sup>** is initially set to 1/m.  \n",
    "* A first predictor is trained and its **weighted error rate r<sub>1</sub>** is computed on the training set; see Equation 7-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/07_03.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **The predictor’s weight α<sub>j</sub>** is then computed using Equation 7-2, where **η is the learning rate hyperparameter** (defaults to 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/07_04.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The more accurate the predictor is, the higher its weight will be. \n",
    "* If it is just guessing randomly, then its weight will be close to zero.\n",
    "* However, if it is most often wrong (i.e., less accurate than random guessing), then its weight will be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/07_05.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The instance weights are updated using Equation 7-3: the misclassified instances are boosted.\n",
    "* Then all the instance weights are normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(i.e., divided by ![Figure 6-2](./img/07_06.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, a new predictor is trained using the updated weights, and the whole process is repeated (the new predictor’s weight is computed, the instance weights are updated, then another predictor is trained, and so on). \n",
    "* The algorithm stops **when the desired number of predictors is reached, or when a perfect predictor is found.**\n",
    "* To make predictions, AdaBoost simply computes the predictions of all the predictors and weighs them using the predictor weights α<sub>j</sub>. \n",
    "* The predicted class is the one that **receives the majority of weighted votes** (see Equation 7-4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/07_07.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scikit-Learn actually uses a multiclass version of AdaBoost called *SAMME* (which stands for *Stagewise Additive Modeling using a Multiclass Exponential loss function*). \n",
    "    * When there are just two classes, SAMME is equivalent to AdaBoost.\n",
    "* Moreover, if the predictors can estimate class probabilities (i.e., if they have a **predict_proba()** method), Scikit-Learn can use a variant of SAMME called *SAMME.R* (the R stands for “Real”), which relies on class probabilities rather than predictions and generally performs better.\n",
    "* The following code trains an AdaBoost classifier based on 200 *Decision Stumps* using Scikit-Learn’s AdaBoostClassifier class (as you might expect, there is also an AdaBoostRegressor class). \n",
    "* A Decision Stump is a Decision Tree with max_depth=1\n",
    "    * In other words, a tree composed of a single decision node plus two leaf nodes. \n",
    "* This is the default base estimator for the AdaBoostClassifier class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_moons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.samples_generator.make_moons(n_samples=100, shuffle=True, noise=None, random_state=None)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 4.81607432e-01, -3.55142763e-01],\n",
       "        [-6.72300890e-01,  7.40277997e-01],\n",
       "        [-5.18392568e-01,  8.55142763e-01],\n",
       "        [ 1.34536505e+00, -4.38468422e-01],\n",
       "        [ 1.03205158e+00, -4.99486216e-01],\n",
       "        [-3.45365054e-01,  9.38468422e-01],\n",
       "        [-9.97945393e-01,  6.40702200e-02],\n",
       "        [ 9.97945393e-01,  6.40702200e-02],\n",
       "        [ 0.00000000e+00,  5.00000000e-01],\n",
       "        [ 9.67948422e-01, -4.99486216e-01],\n",
       "        [ 9.03976974e-01, -4.95379113e-01],\n",
       "        [ 9.90311321e-02,  6.61162609e-02],\n",
       "        [ 7.77479066e-01, -4.74927912e-01],\n",
       "        [ 1.62348980e+00, -2.81831482e-01],\n",
       "        [ 8.71318704e-01,  4.90717552e-01],\n",
       "        [ 1.09602303e+00, -4.95379113e-01],\n",
       "        [ 6.54634946e-01, -4.38468422e-01],\n",
       "        [-9.60230259e-02,  9.95379113e-01],\n",
       "        [ 3.45365054e-01,  9.38468422e-01],\n",
       "        [-4.62538290e-01,  8.86599306e-01],\n",
       "        [-9.67294863e-01,  2.53654584e-01],\n",
       "        [ 1.57211666e+00, -3.20172255e-01],\n",
       "        [ 7.15472413e-01, -4.58667853e-01],\n",
       "        [ 1.99794539e+00,  4.35929780e-01],\n",
       "        [ 9.49055747e-01,  3.15108218e-01],\n",
       "        [ 1.67230089e+00, -2.40277997e-01],\n",
       "        [-9.91790014e-01,  1.27877162e-01],\n",
       "        [ 3.27699110e-01, -2.40277997e-01],\n",
       "        [ 1.46253829e+00, -3.86599306e-01],\n",
       "        [ 1.87131870e+00,  9.28244800e-03],\n",
       "        [ 4.04783343e-01,  9.14412623e-01],\n",
       "        [ 3.27051370e-02,  2.46345416e-01],\n",
       "        [ 5.37461710e-01, -3.86599306e-01],\n",
       "        [ 1.94905575e+00,  1.84891782e-01],\n",
       "        [ 9.00968868e-01,  4.33883739e-01],\n",
       "        [ 9.81559157e-01,  1.91158629e-01],\n",
       "        [ 2.05460725e-03,  4.35929780e-01],\n",
       "        [ 3.20515776e-02,  9.99486216e-01],\n",
       "        [ 7.61445958e-01,  6.48228395e-01],\n",
       "        [ 1.98586378e-01, -9.81105305e-02],\n",
       "        [ 1.71834935e+00, -1.95682551e-01],\n",
       "        [ 1.99179001e+00,  3.72122838e-01],\n",
       "        [ 1.15959990e+00, -4.87181783e-01],\n",
       "        [-2.22520934e-01,  9.74927912e-01],\n",
       "        [-9.00968868e-01,  4.33883739e-01],\n",
       "        [ 5.95216657e-01, -4.14412623e-01],\n",
       "        [ 9.60230259e-02,  9.95379113e-01],\n",
       "        [ 9.26916757e-01,  3.75267005e-01],\n",
       "        [ 2.22520934e-01,  9.74927912e-01],\n",
       "        [ 1.96729486e+00,  2.46345416e-01],\n",
       "        [ 4.27883340e-01, -3.20172255e-01],\n",
       "        [ 1.80141362e+00, -9.81105305e-02],\n",
       "        [-9.26916757e-01,  3.75267005e-01],\n",
       "        [-4.04783343e-01,  9.14412623e-01],\n",
       "        [-3.20515776e-02,  9.99486216e-01],\n",
       "        [-8.01413622e-01,  5.98110530e-01],\n",
       "        [-9.81559157e-01,  1.91158629e-01],\n",
       "        [ 9.91790014e-01,  1.27877162e-01],\n",
       "        [ 6.72300890e-01,  7.40277997e-01],\n",
       "        [ 1.90096887e+00,  6.61162609e-02],\n",
       "        [ 1.76144596e+00, -1.48228395e-01],\n",
       "        [ 1.28681296e-01,  9.28244800e-03],\n",
       "        [ 1.84408430e-02,  3.08841371e-01],\n",
       "        [-2.84527587e-01,  9.58667853e-01],\n",
       "        [-8.71318704e-01,  4.90717552e-01],\n",
       "        [ 1.00000000e+00,  0.00000000e+00],\n",
       "        [ 5.18392568e-01,  8.55142763e-01],\n",
       "        [ 7.18349350e-01,  6.95682551e-01],\n",
       "        [ 5.72116660e-01,  8.20172255e-01],\n",
       "        [-9.49055747e-01,  3.15108218e-01],\n",
       "        [ 1.22252093e+00, -4.74927912e-01],\n",
       "        [ 8.20998618e-03,  3.72122838e-01],\n",
       "        [ 2.38554042e-01, -1.48228395e-01],\n",
       "        [ 1.51839257e+00, -3.55142763e-01],\n",
       "        [ 1.98155916e+00,  3.08841371e-01],\n",
       "        [ 1.28452759e+00, -4.58667853e-01],\n",
       "        [-7.61445958e-01,  6.48228395e-01],\n",
       "        [ 8.01413622e-01,  5.98110530e-01],\n",
       "        [ 1.59599895e-01,  9.87181783e-01],\n",
       "        [ 6.23489802e-01,  7.81831482e-01],\n",
       "        [-8.38088105e-01,  5.45534901e-01],\n",
       "        [-1.59599895e-01,  9.87181783e-01],\n",
       "        [ 5.09442530e-02,  1.84891782e-01],\n",
       "        [-6.23489802e-01,  7.81831482e-01],\n",
       "        [ 8.40400105e-01, -4.87181783e-01],\n",
       "        [ 9.67294863e-01,  2.53654584e-01],\n",
       "        [-7.18349350e-01,  6.95682551e-01],\n",
       "        [-1.00000000e+00,  1.22464680e-16],\n",
       "        [ 1.92691676e+00,  1.24732995e-01],\n",
       "        [ 3.76510198e-01, -2.81831482e-01],\n",
       "        [ 1.40478334e+00, -4.14412623e-01],\n",
       "        [ 2.81650650e-01, -1.95682551e-01],\n",
       "        [ 1.61911895e-01, -4.55349012e-02],\n",
       "        [ 4.62538290e-01,  8.86599306e-01],\n",
       "        [ 8.38088105e-01,  5.45534901e-01],\n",
       "        [ 2.00000000e+00,  5.00000000e-01],\n",
       "        [ 2.84527587e-01,  9.58667853e-01],\n",
       "        [-5.72116660e-01,  8.20172255e-01],\n",
       "        [ 7.30832427e-02,  1.24732995e-01],\n",
       "        [ 1.83808810e+00, -4.55349012e-02]]),\n",
       " array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(\n",
    "            DecisionTreeClassifier(max_depth=1),\n",
    "            n_estimators=200,\n",
    "            algorithm=\"SAMME.R\",\n",
    "            learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=1,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort=False,\n",
       "                                                         random_state=None,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.predict([[0.5,0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your AdaBoost ensemble is overfitting the training set, you can try **reducing the number of estimators** or **more strongly regularizing the base estimator.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another very popular Boosting algorithm is **Gradient Boosting**. \n",
    "* Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. \n",
    "* However, instead of tweaking the instance weights at every iteration like AdaBoost does, **this method tries to fit the new predictor to the residual errors made by the previous predictor.**\n",
    "* Let’s go through a simple regression example using Decision Trees as the base predictors (of course Gradient Boosting also works great with regression tasks). \n",
    "* This is called *Gradient Tree Boosting*, or *Gradient Boosted Regression Trees (GBRT)*. \n",
    "* First, let’s fit a DecisionTreeRegressor to the training set (for example, a noisy quadratic training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a second DecisionTreeRegressor on the residual errors made by the first predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train a third regressor on the residual errors made by the second predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an ensemble containing three trees.  \n",
    "It can make predictions on a new instance simply by adding up the predictions of all the trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict([[2,0.99]]) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88461538])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/07_08.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Figure 7-9 represents the predictions of these three trees in the left column, and the ensemble’s predictions in the right column. \n",
    "    * In the first row, the ensemble has just one tree, so its predictions are exactly the same as the first tree’s predictions. \n",
    "    * In the second row, a new tree is trained on the residual errors of the first tree.\n",
    "    * On the right you can see that the ensemble’s predictions are equal to the sum of the predictions of the first two trees. \n",
    "    * Similarly, in the third row another tree is trained on the residual errors of the second tree. \n",
    "    * You can see that the ensemble’s predictions gradually get better as trees are added to the ensemble.\n",
    "\n",
    "<br>\n",
    "\n",
    "* A simpler way to train GBRT ensembles is to use Scikit-Learn’s *GradientBoostingRegressor* class. \n",
    "* Much like the RandomForestRegressor class, it has hyperparameters to control the growth of Decision Trees (e.g., max_depth, min_samples_leaf, and so on), as well as hyperparameters to control the ensemble training, such as the number of trees (n_estimators). \n",
    "* The following code creates the same ensemble as the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=1.0, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=3,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The *learning_rat*e hyperparameter scales the contribution of each tree. \n",
    "    * If you set it to a low value, such as 0.1, you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. \n",
    "* This is a regularization technique called **shrinkage**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/07_09.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Figure 7-10 shows two GBRT ensembles trained with a low learning rate: \n",
    "    * the one on the left does not have enough trees to fit the training set,\n",
    "    * while the one on the right has too many trees and overfits the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to find the optimal number of trees, you can use early stopping (see Chapter 4). \n",
    "* A simple way to implement this is to use the *staged_predict()* method:\n",
    "    * it returns an iterator over the predictions made by the ensemble at each stage of training (with one tree, two trees, etc.). \n",
    "* The following code trains a GBRT ensemble with 120 trees, then measures the validation error at each stage of training to find the optimal number of trees, and finally trains another GBRT ensemble using the optimal number of trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 2)\n",
      "(75,)\n",
      "(25, 2)\n",
      "(25,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=120,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "          for y_pred in gbrt.staged_predict(X_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.23238276440376182,\n",
       " 0.20520185375519062,\n",
       " 0.1817077849458973,\n",
       " 0.16396975330990482,\n",
       " 0.15033928039124006,\n",
       " 0.13756787904158294,\n",
       " 0.12723950776027335,\n",
       " 0.1185297248976464,\n",
       " 0.10971371888891925,\n",
       " 0.10378305075899451,\n",
       " 0.09915788652829102,\n",
       " 0.09354451078104026,\n",
       " 0.09046391374451458,\n",
       " 0.09042533714502157,\n",
       " 0.0882195461968501,\n",
       " 0.08654023471338698,\n",
       " 0.08377402515928267,\n",
       " 0.08496718241017817,\n",
       " 0.08699624951509458,\n",
       " 0.08394975347957175,\n",
       " 0.08286519086369207,\n",
       " 0.07700649093529138,\n",
       " 0.07449857468217595,\n",
       " 0.0732159674370101,\n",
       " 0.07247582942931458,\n",
       " 0.07045167557711193,\n",
       " 0.06573582789892654,\n",
       " 0.06485453910284955,\n",
       " 0.06102975192821259,\n",
       " 0.05976172930727478,\n",
       " 0.05412256756865735,\n",
       " 0.052750250264340146,\n",
       " 0.05211702047113812,\n",
       " 0.04944743425350511,\n",
       " 0.05071141608764854,\n",
       " 0.04987457073331363,\n",
       " 0.049571289249545375,\n",
       " 0.04553191579163353,\n",
       " 0.04463756020531218,\n",
       " 0.04366662141926428,\n",
       " 0.04186461499990637,\n",
       " 0.04157574056400914,\n",
       " 0.04104529791628604,\n",
       " 0.03807905347087447,\n",
       " 0.03739119154756834,\n",
       " 0.03494059358105625,\n",
       " 0.03428853583313858,\n",
       " 0.03393751188776223,\n",
       " 0.03288613838060827,\n",
       " 0.03230436595430136,\n",
       " 0.031751380706788596,\n",
       " 0.029919473868965363,\n",
       " 0.02959978469245837,\n",
       " 0.02808296798264573,\n",
       " 0.027573882021887403,\n",
       " 0.0272591263475272,\n",
       " 0.026718741976046526,\n",
       " 0.026207599792801503,\n",
       " 0.025784115874650855,\n",
       " 0.025512224802925188,\n",
       " 0.025094742833732874,\n",
       " 0.02474162423797558,\n",
       " 0.02441445167638997,\n",
       " 0.02411994983290156,\n",
       " 0.023756465559535665,\n",
       " 0.023673140581288377,\n",
       " 0.02340253492137234,\n",
       " 0.022878763823367784,\n",
       " 0.022608357447296792,\n",
       " 0.022397217056789844,\n",
       " 0.022217236827425265,\n",
       " 0.021298887392639833,\n",
       " 0.02126203308660347,\n",
       " 0.02101889836479212,\n",
       " 0.020740329617789287,\n",
       " 0.02035987906596288,\n",
       " 0.02024813361842338,\n",
       " 0.02003721875633213,\n",
       " 0.02003901905937601,\n",
       " 0.019886701692425294,\n",
       " 0.01955623089695361,\n",
       " 0.019558586687703758,\n",
       " 0.019420820228739814,\n",
       " 0.018809061184429602,\n",
       " 0.01867298109250396,\n",
       " 0.018640445366293817,\n",
       " 0.018545381657768714,\n",
       " 0.018548883319799026,\n",
       " 0.018303949286259526,\n",
       " 0.01815764654013913,\n",
       " 0.018132148283000825,\n",
       " 0.018007137710318335,\n",
       " 0.017981472299259622,\n",
       " 0.017911518639745914,\n",
       " 0.017914331200125654,\n",
       " 0.017822500777689418,\n",
       " 0.01760451023787942,\n",
       " 0.017480047853629645,\n",
       " 0.01749926397083159,\n",
       " 0.017416684001503743,\n",
       " 0.0174212506668974,\n",
       " 0.017261174071128437,\n",
       " 0.017169445577549198,\n",
       " 0.017070451706194138,\n",
       " 0.017052395022827786,\n",
       " 0.016971690795009983,\n",
       " 0.01695059187761906,\n",
       " 0.01691078123646599,\n",
       " 0.016916395465623468,\n",
       " 0.01685938631744146,\n",
       " 0.01649595009945949,\n",
       " 0.016456082408371823,\n",
       " 0.016470140609753416,\n",
       " 0.016454722990446025,\n",
       " 0.016437853490515696,\n",
       " 0.0163906862397835,\n",
       " 0.01608148968889711,\n",
       " 0.01600883060047742,\n",
       " 0.015747044637171213,\n",
       " 0.01567201452838877]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/07_11.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 2)\n",
      "[0.5649697  0.47610811 0.47610811 0.5649697  0.47610811 0.47610811\n",
      " 0.47610811 0.5649697  0.47610811 0.47610811 0.47610811 0.47610811\n",
      " 0.47610811 0.47610811 0.47610811 0.5649697  0.5649697  0.47610811\n",
      " 0.568      0.568      0.47610811 0.468      0.5649697  0.5649697\n",
      " 0.47610811]\n",
      "[0.60544242 0.43660541 0.43660541 0.60544242 0.43660541 0.43660541\n",
      " 0.43660541 0.60544242 0.43660541 0.43660541 0.43660541 0.43660541\n",
      " 0.43660541 0.43660541 0.43660541 0.60544242 0.60544242 0.43660541\n",
      " 0.6112     0.6112     0.43660541 0.4212     0.60544242 0.60544242\n",
      " 0.43660541]\n",
      "[0.64026462 0.39882722 0.39882722 0.64026462 0.39882722 0.39882722\n",
      " 0.39882722 0.64026462 0.39882722 0.39882722 0.39882722 0.39882722\n",
      " 0.39882722 0.39882722 0.39882722 0.64026462 0.64026462 0.39882722\n",
      " 0.65008    0.65008    0.39882722 0.37830973 0.64026462 0.64026462\n",
      " 0.39882722]\n",
      "[0.67320785 0.36667399 0.36667399 0.67320785 0.36667399 0.36667399\n",
      " 0.36667399 0.67320785 0.36667399 0.36667399 0.36667399 0.36667399\n",
      " 0.36667399 0.36667399 0.36667399 0.67320785 0.67320785 0.36667399\n",
      " 0.68517345 0.68517345 0.36667399 0.34047876 0.67320785 0.67320785\n",
      " 0.36667399]\n",
      "[0.70160918 0.33588894 0.33588894 0.70160918 0.33588894 0.33588894\n",
      " 0.33588894 0.70160918 0.33588894 0.33588894 0.33588894 0.33588894\n",
      " 0.33588894 0.33588894 0.33588894 0.70160918 0.70160918 0.33588894\n",
      " 0.7166561  0.7166561  0.33588894 0.30537672 0.70160918 0.70160918\n",
      " 0.33588894]\n",
      "[0.72677224 0.30563338 0.30563338 0.72677224 0.30563338 0.30563338\n",
      " 0.30563338 0.72677224 0.361052   0.30563338 0.30563338 0.30563338\n",
      " 0.30563338 0.30563338 0.361052   0.72677224 0.72677224 0.30563338\n",
      " 0.74499049 0.74181916 0.30563338 0.27278636 0.72677224 0.72677224\n",
      " 0.30563338]\n",
      "[0.75106471 0.28191108 0.28191108 0.75106471 0.28191108 0.28191108\n",
      " 0.28191108 0.75106471 0.33732971 0.28191108 0.28191108 0.28191108\n",
      " 0.28191108 0.28191108 0.33732971 0.75106471 0.75106471 0.28191108\n",
      " 0.77082849 0.76765716 0.28191108 0.24550773 0.75106471 0.75106471\n",
      " 0.28191108]\n",
      "[0.77176676 0.25705331 0.25705331 0.77176676 0.25705331 0.25705331\n",
      " 0.25705331 0.77176676 0.35803175 0.25705331 0.25705331 0.25705331\n",
      " 0.25705331 0.25705331 0.35803175 0.77176676 0.77176676 0.25705331\n",
      " 0.79374564 0.7883592  0.25705331 0.21863754 0.77176676 0.77176676\n",
      " 0.25705331]\n",
      "[0.78876834 0.23134798 0.23134798 0.78876834 0.23134798 0.23134798\n",
      " 0.23134798 0.78876834 0.37503334 0.23134798 0.27405489 0.23134798\n",
      " 0.27405489 0.23221778 0.37503334 0.78876834 0.78876834 0.23134798\n",
      " 0.81074722 0.80536079 0.23134798 0.19380201 0.78876834 0.78876834\n",
      " 0.27405489]\n",
      "[0.80989151 0.2136405  0.2136405  0.80989151 0.2136405  0.2136405\n",
      " 0.2136405  0.79232858 0.35732586 0.2136405  0.25634742 0.2136405\n",
      " 0.25634742 0.21451031 0.35732586 0.80989151 0.80989151 0.2136405\n",
      " 0.8301203  0.82473387 0.2136405  0.19736226 0.80989151 0.80989151\n",
      " 0.25634742]\n",
      "[0.82405969 0.19227645 0.19227645 0.82405969 0.19227645 0.19227645\n",
      " 0.19227645 0.80649677 0.37149405 0.19227645 0.27051561 0.19227645\n",
      " 0.27051561 0.19362526 0.37149405 0.82405969 0.82405969 0.19227645\n",
      " 0.84428849 0.83890205 0.19227645 0.17647721 0.82405969 0.82405969\n",
      " 0.27051561]\n",
      "[0.83818471 0.17532546 0.17532546 0.83818471 0.17532546 0.17532546\n",
      " 0.17532546 0.82062179 0.38561906 0.17532546 0.25356462 0.17532546\n",
      " 0.25356462 0.17667427 0.38561906 0.83818471 0.83818471 0.17532546\n",
      " 0.85985964 0.85302707 0.17532546 0.15810515 0.83818471 0.83818471\n",
      " 0.25356462]\n",
      "[0.85436624 0.16264285 0.16264285 0.85436624 0.16264285 0.16264285\n",
      " 0.16264285 0.80793917 0.37293645 0.16264285 0.240882   0.16264285\n",
      " 0.240882   0.16399166 0.37293645 0.85436624 0.85436624 0.16264285\n",
      " 0.8758525  0.86901993 0.16264285 0.14542254 0.85436624 0.85436624\n",
      " 0.240882  ]\n",
      "[0.8651294  0.14637856 0.14637856 0.8651294  0.14637856 0.14637856\n",
      " 0.14637856 0.81870234 0.38369961 0.14637856 0.25164517 0.14637856\n",
      " 0.25164517 0.14823968 0.38369961 0.8651294  0.8651294  0.14637856\n",
      " 0.88661567 0.8797831  0.14637856 0.12967056 0.8651294  0.8651294\n",
      " 0.25164517]\n",
      "[0.87861646 0.13580989 0.13580989 0.87861646 0.13580989 0.13580989\n",
      " 0.13580989 0.80813366 0.37313094 0.13580989 0.24107649 0.13580989\n",
      " 0.24107649 0.137671   0.37313094 0.87861646 0.87861646 0.13580989\n",
      " 0.89993293 0.89310036 0.13580989 0.11910188 0.87861646 0.87861646\n",
      " 0.24107649]\n",
      "[0.88759873 0.1222289  0.1222289  0.88759873 0.1222289  0.1222289\n",
      " 0.1222289  0.81711593 0.38211321 0.1222289  0.25005876 0.1222289\n",
      " 0.25005876 0.12455109 0.38211321 0.88759873 0.88759873 0.1222289\n",
      " 0.9089152  0.90208263 0.1222289  0.10598197 0.88759873 0.88759873\n",
      " 0.25005876]\n",
      "[0.89883886 0.117975   0.117975   0.89883886 0.117975   0.117975\n",
      " 0.117975   0.81286203 0.3778593  0.117975   0.24580486 0.117975\n",
      " 0.24580486 0.12029718 0.3778593  0.89883886 0.89883886 0.117975\n",
      " 0.90466129 0.89782872 0.117975   0.10172807 0.89883886 0.89883886\n",
      " 0.24580486]\n",
      "[0.90256664 0.1061775  0.1061775  0.90256664 0.1061775  0.1061775\n",
      " 0.1061775  0.81658981 0.38158709 0.1061775  0.28828155 0.1061775\n",
      " 0.28828155 0.16277388 0.38158709 0.90256664 0.90256664 0.1061775\n",
      " 0.94713798 0.9015565  0.1061775  0.10545585 0.90256664 0.90256664\n",
      " 0.24953264]\n",
      "[0.90741959 0.09667404 0.09667404 0.90741959 0.09667404 0.09667404\n",
      " 0.09667404 0.82144276 0.38644003 0.09667404 0.27877809 0.09667404\n",
      " 0.27877809 0.15327042 0.38644003 0.90741959 0.90741959 0.09667404\n",
      " 0.95436163 0.90640945 0.09667404 0.11030879 0.90741959 0.90741959\n",
      " 0.24002918]\n",
      "[0.91667763 0.09335672 0.09335672 0.91667763 0.09335672 0.09335672\n",
      " 0.09335672 0.81812543 0.38312271 0.09335672 0.27546077 0.09335672\n",
      " 0.27546077 0.1499531  0.38312271 0.91667763 0.91667763 0.09335672\n",
      " 0.95104431 0.90309213 0.09335672 0.10699147 0.91667763 0.91667763\n",
      " 0.23671186]\n",
      "[0.92077018 0.08513534 0.08513534 0.92077018 0.08513534 0.08513534\n",
      " 0.08513534 0.82221799 0.38721527 0.08513534 0.26723939 0.08513534\n",
      " 0.26723939 0.14173172 0.38721527 0.92077018 0.92077018 0.08513534\n",
      " 0.95787732 0.90718468 0.08513534 0.11108403 0.92077018 0.92077018\n",
      " 0.22849048]\n",
      "[0.92537245 0.08112461 0.08112461 0.92537245 0.08112461 0.08112461\n",
      " 0.08112461 0.83736812 0.38320453 0.08112461 0.26322866 0.08112461\n",
      " 0.26322866 0.13772099 0.38320453 0.92537245 0.92537245 0.08112461\n",
      " 0.95386659 0.90317395 0.08112461 0.12623416 0.92537245 0.92537245\n",
      " 0.22447975]\n",
      "[0.93283521 0.07865701 0.07865701 0.93283521 0.07865701 0.07865701\n",
      " 0.07865701 0.83490052 0.38073693 0.07865701 0.26076106 0.07865701\n",
      " 0.26076106 0.13525339 0.38073693 0.93283521 0.93283521 0.07865701\n",
      " 0.95139899 0.90070635 0.07865701 0.12376656 0.93283521 0.93283521\n",
      " 0.22201215]\n",
      "[0.93487775 0.07079131 0.07079131 0.93487775 0.07079131 0.07079131\n",
      " 0.07079131 0.83694306 0.38277948 0.07079131 0.30015306 0.07079131\n",
      " 0.30015306 0.17464539 0.38277948 0.93487775 0.93487775 0.07079131\n",
      " 0.99079099 0.90274889 0.07079131 0.1258091  0.93487775 0.93487775\n",
      " 0.22405469]\n",
      "[0.93831664 0.06724382 0.06724382 0.93831664 0.06724382 0.06724382\n",
      " 0.06724382 0.84038195 0.36205689 0.06724382 0.29660557 0.06724382\n",
      " 0.29660557 0.1710979  0.36205689 0.93831664 0.93831664 0.06724382\n",
      " 0.9872435  0.88202631 0.06724382 0.12924799 0.93831664 0.93831664\n",
      " 0.22050721]\n",
      "[0.94096427 0.06392199 0.06392199 0.94096427 0.06392199 0.06392199\n",
      " 0.06392199 0.84302959 0.36470452 0.06392199 0.29328375 0.06392199\n",
      " 0.29328375 0.16777607 0.36470452 0.94096427 0.94096427 0.06392199\n",
      " 0.98392167 0.88467394 0.06392199 0.13189562 0.94096427 0.94096427\n",
      " 0.20825485]\n",
      "[0.94686785 0.06218544 0.06218544 0.94686785 0.06218544 0.06218544\n",
      " 0.06218544 0.84129303 0.36296797 0.06218544 0.29154719 0.06218544\n",
      " 0.29154719 0.16603952 0.36296797 0.94686785 0.94686785 0.06218544\n",
      " 0.98218512 0.88293739 0.06218544 0.13015907 0.94686785 0.94686785\n",
      " 0.20651829]\n",
      "[0.94827584 0.05596689 0.05596689 0.94827584 0.05596689 0.05596689\n",
      " 0.05596689 0.84270103 0.36437596 0.05596689 0.32786057 0.05596689\n",
      " 0.32786057 0.2023529  0.36437596 0.94827584 0.94827584 0.05596689\n",
      " 1.0184985  0.88434538 0.05596689 0.13156706 0.94827584 0.94827584\n",
      " 0.20792629]\n",
      "[0.95075848 0.05544358 0.05544358 0.95075848 0.05544358 0.05544358\n",
      " 0.05544358 0.85580285 0.3497783  0.05544358 0.31326291 0.05544358\n",
      " 0.32733727 0.20182959 0.36385266 0.95075848 0.95075848 0.05544358\n",
      " 1.00390084 0.86974772 0.05544358 0.14466889 0.95075848 0.95075848\n",
      " 0.20740298]\n",
      "[0.95197136 0.04989923 0.04989923 0.95197136 0.04989923 0.04989923\n",
      " 0.04989923 0.85701573 0.35099118 0.04989923 0.34670101 0.04989923\n",
      " 0.36077536 0.23526769 0.36506554 0.95197136 0.95197136 0.04989923\n",
      " 1.03733894 0.8709606  0.04989923 0.14588177 0.95197136 0.95197136\n",
      " 0.20861586]\n",
      "[0.95677422 0.05300567 0.05300567 0.95677422 0.04041067 0.04041067\n",
      " 0.04041067 0.84752717 0.34150262 0.05300567 0.33721245 0.05300567\n",
      " 0.3638818  0.23837413 0.36817198 0.95677422 0.95677422 0.04041067\n",
      " 1.02785038 0.86147204 0.05300567 0.14898821 0.95677422 0.95677422\n",
      " 0.2117223 ]\n",
      "[0.95679012 0.05302156 0.04635952 0.95679012 0.04042656 0.04042656\n",
      " 0.04042656 0.84754307 0.34151852 0.04635952 0.33722834 0.04635952\n",
      " 0.42207188 0.23172798 0.36818787 0.95679012 0.95679012 0.04042656\n",
      " 1.02786627 0.86148794 0.04635952 0.14234206 0.95679012 0.95679012\n",
      " 0.26991238]\n",
      "[0.9574139  0.05364535 0.0469833  0.9574139  0.04105035 0.04105035\n",
      " 0.04105035 0.79801115 0.2919866  0.0469833  0.33785213 0.0469833\n",
      " 0.42269566 0.23235176 0.36881166 0.96251361 0.9574139  0.04105035\n",
      " 1.03358976 0.86721143 0.0469833  0.14296584 0.9574139  0.9072582\n",
      " 0.27053616]\n",
      "[0.95745247 0.05368391 0.04093939 0.95745247 0.04108891 0.04108891\n",
      " 0.04108891 0.79804972 0.29202517 0.04093939 0.33789069 0.04093939\n",
      " 0.47500436 0.22630785 0.36885022 0.96255218 0.95745247 0.04108891\n",
      " 1.03362833 0.86724999 0.04093939 0.13692193 0.95745247 0.90729677\n",
      " 0.32284485]\n",
      "[0.95918374 0.05167066 0.03892614 0.95918374 0.03907566 0.03907566\n",
      " 0.03907566 0.79978098 0.27610598 0.03892614 0.33587744 0.03892614\n",
      " 0.47299111 0.2242946  0.35293103 0.96428344 0.95918374 0.03907566\n",
      " 1.03161508 0.8513308  0.03892614 0.1386532  0.95918374 0.90902804\n",
      " 0.3208316 ]\n",
      "[0.95982628 0.05231321 0.03956869 0.95982628 0.03971821 0.03971821\n",
      " 0.03971821 0.75561362 0.23193862 0.03956869 0.33651999 0.03956869\n",
      " 0.47363365 0.22493715 0.35357358 0.96917322 0.95982628 0.03971821\n",
      " 1.02930458 0.85622057 0.03956869 0.13929574 0.95982628 0.86486068\n",
      " 0.32147415]\n",
      "[0.97126182 0.05058784 0.03784331 0.95999659 0.03799284 0.03799284\n",
      " 0.03799284 0.75578393 0.23210892 0.03784331 0.33479462 0.03784331\n",
      " 0.47190828 0.22321178 0.36500911 0.96934352 0.95999659 0.03799284\n",
      " 1.02757921 0.85639088 0.03784331 0.15073128 0.95999659 0.86503098\n",
      " 0.31210615]\n",
      "[0.97121789 0.05054391 0.03405898 0.95995266 0.03794891 0.03794891\n",
      " 0.03794891 0.75574    0.232065   0.03405898 0.33475069 0.03405898\n",
      " 0.51863767 0.21942744 0.36496518 0.9692996  0.95995266 0.03794891\n",
      " 1.02753529 0.85634695 0.03405898 0.14420631 0.95995266 0.86498706\n",
      " 0.35883554]\n",
      "[0.97253199 0.05185802 0.03537309 0.96126677 0.03926301 0.03926301\n",
      " 0.03926301 0.7248841  0.2012091  0.03537309 0.3360648  0.03537309\n",
      " 0.51995177 0.22074155 0.36627929 0.97236964 0.96126677 0.03926301\n",
      " 1.03398557 0.86279724 0.03537309 0.14552042 0.96126677 0.83413116\n",
      " 0.36014964]\n",
      "[0.97338597 0.04820113 0.0317162  0.96212074 0.03560613 0.03560613\n",
      " 0.03560613 0.72573808 0.20206307 0.0317162  0.33240791 0.03073678\n",
      " 0.51629488 0.21708466 0.36713326 0.97322361 0.96212074 0.03560613\n",
      " 1.0819904  0.86365121 0.0317162  0.14637439 0.96212074 0.83498513\n",
      " 0.36100362]\n",
      "[0.97272335 0.04753851 0.02855274 0.96145812 0.03494351 0.03494351\n",
      " 0.03494351 0.72507546 0.20140045 0.02855274 0.33174529 0.02757332\n",
      " 0.54505553 0.2139212  0.39589391 0.97256099 0.96145812 0.03494351\n",
      " 1.08132778 0.86298859 0.02855274 0.14028512 0.96145812 0.83432251\n",
      " 0.38976427]\n",
      "[0.9737906  0.04630183 0.02731606 0.96252537 0.03370683 0.03370683\n",
      " 0.03370683 0.73530912 0.20016378 0.02731606 0.33050861 0.02633665\n",
      " 0.54381886 0.21268452 0.39465724 0.97362824 0.96252537 0.03370683\n",
      " 1.0800911  0.86175192 0.02731606 0.15051877 0.96252537 0.83538976\n",
      " 0.38852759]\n",
      "[0.98010091 0.04483926 0.02585349 0.96883568 0.03224426 0.03224426\n",
      " 0.03224426 0.73357178 0.19842644 0.02585349 0.32904604 0.02487408\n",
      " 0.54235629 0.21122195 0.40096755 0.9718909  0.96883568 0.03224426\n",
      " 1.07862853 0.86001458 0.02585349 0.15682908 0.96883568 0.83365242\n",
      " 0.38143001]\n",
      "[0.98060356 0.04534192 0.02635615 0.96933834 0.03274691 0.03274691\n",
      " 0.03274691 0.69777192 0.16262659 0.02635615 0.3295487  0.02537673\n",
      " 0.54285894 0.21172461 0.4014702  0.97470181 0.96933834 0.03274691\n",
      " 1.08435756 0.86574361 0.02635615 0.15733174 0.96933834 0.79785257\n",
      " 0.38193266]\n",
      "[0.9807282  0.04546656 0.02648079 0.96946298 0.03287156 0.03287156\n",
      " 0.03287156 0.65757346 0.12242812 0.02648079 0.32967334 0.02550137\n",
      " 0.54298358 0.21184925 0.40159484 0.9783427  0.96946298 0.03287156\n",
      " 1.08193898 0.8693845  0.02648079 0.15745638 0.96946298 0.79797721\n",
      " 0.38205731]\n",
      "[0.98947712 0.0407133  0.02618576 0.96952275 0.0281183  0.0281183\n",
      " 0.0281183  0.65763323 0.12248789 0.02618576 0.32492008 0.02520634\n",
      " 0.54268855 0.21155422 0.41034376 0.97840248 0.96952275 0.0281183\n",
      " 1.07718573 0.86944427 0.02618576 0.1662053  0.96952275 0.79803699\n",
      " 0.38176228]\n",
      "[0.98889444 0.04013062 0.02357534 0.96894007 0.02753562 0.02753562\n",
      " 0.02753562 0.65705055 0.12190521 0.02357534 0.3243374  0.02259593\n",
      " 0.56808045 0.20584806 0.43573566 0.97781979 0.96894007 0.02753562\n",
      " 1.07660305 0.86886159 0.02357534 0.16049914 0.96894007 0.7974543\n",
      " 0.40715417]\n",
      "[0.98992376 0.03889983 0.02234455 0.96996939 0.02630483 0.02630483\n",
      " 0.02630483 0.65807986 0.11068215 0.02234455 0.32310661 0.02136514\n",
      " 0.56684966 0.20461727 0.4245126  0.97884911 0.96996939 0.02630483\n",
      " 1.07537226 0.85763853 0.02234455 0.16152846 0.96996939 0.79848362\n",
      " 0.40592338]\n",
      "[0.99092831 0.03990438 0.02334911 0.97097394 0.02730938 0.02730938\n",
      " 0.02730938 0.63536764 0.08796993 0.02334911 0.32411116 0.02236969\n",
      " 0.56785421 0.20562182 0.42551715 0.9809642  0.97097394 0.02730938\n",
      " 1.08017458 0.86244085 0.02334911 0.16253301 0.97097394 0.7757714\n",
      " 0.40692793]\n",
      "[0.99152383 0.03735187 0.0207966  0.97156946 0.02475687 0.02475687\n",
      " 0.02475687 0.63596316 0.08856545 0.0207966  0.32155865 0.01981718\n",
      " 0.5653017  0.20306931 0.42611267 0.98155972 0.97156946 0.02475687\n",
      " 1.11822298 0.86303637 0.0207966  0.16312853 0.97156946 0.77636692\n",
      " 0.40752346]\n",
      "[0.99145132 0.03727936 0.01875245 0.97149695 0.02468436 0.02468436\n",
      " 0.02468436 0.63589065 0.08849294 0.01875245 0.32148614 0.01777303\n",
      " 0.59685483 0.20102517 0.42604016 0.98148721 0.97149695 0.02468436\n",
      " 1.11815047 0.86296386 0.01875245 0.15817698 0.97149695 0.77629441\n",
      " 0.43907658]\n",
      "[0.99230589 0.03314161 0.01926747 0.97235152 0.02054661 0.02054661\n",
      " 0.02054661 0.63674522 0.08435519 0.01926747 0.31734839 0.01828806\n",
      " 0.59736985 0.20154019 0.42190241 0.98234178 0.97235152 0.02054661\n",
      " 1.11401272 0.85882611 0.01926747 0.15903156 0.97235152 0.77714898\n",
      " 0.43959161]\n",
      "[0.99226192 0.03309764 0.01737624 0.97230755 0.02050264 0.02050264\n",
      " 0.02050264 0.63670125 0.08431121 0.01737624 0.31730442 0.01639682\n",
      " 0.62571617 0.19964896 0.42185844 0.98229781 0.97230755 0.02050264\n",
      " 1.11396875 0.85878214 0.01737624 0.15451396 0.97230755 0.77710501\n",
      " 0.46793792]\n",
      "[0.99249931 0.03333503 0.01761363 0.97254494 0.02074003 0.02074003\n",
      " 0.02074003 0.60260985 0.05021981 0.01761363 0.31754181 0.01663421\n",
      " 0.62595356 0.19988635 0.42209583 0.98406803 0.97254494 0.02074003\n",
      " 1.11818626 0.86299965 0.01761363 0.15475135 0.97254494 0.7773424\n",
      " 0.46817531]\n",
      "[0.99336762 0.03230287 0.01658148 0.97341325 0.01970787 0.01970787\n",
      " 0.01970787 0.60347816 0.04146462 0.01658148 0.31650965 0.01560206\n",
      " 0.6249214  0.19885419 0.41334064 0.98493634 0.97341325 0.01970787\n",
      " 1.11715411 0.85424446 0.01658148 0.15561965 0.97341325 0.77821071\n",
      " 0.46714316]\n",
      "[0.99851051 0.03744576 0.01414928 0.97855614 0.02485076 0.02485076\n",
      " 0.02485076 0.59505516 0.03304162 0.01414928 0.30808665 0.01316987\n",
      " 0.63006429 0.196422   0.41848353 0.98710497 0.97855614 0.02485076\n",
      " 1.11932275 0.8564131  0.01414928 0.15318746 0.97855614 0.76978771\n",
      " 0.47228605]\n",
      "[1.00303639 0.03544589 0.01214941 0.98308202 0.02285089 0.02285089\n",
      " 0.02285089 0.58062128 0.01860774 0.01214941 0.29365278 0.01116999\n",
      " 0.62806442 0.20094788 0.42300941 0.98905675 0.98308202 0.02285089\n",
      " 1.12127452 0.85836487 0.01214941 0.15771334 0.98308202 0.75535383\n",
      " 0.47681193]\n",
      "[1.00710968 0.033646   0.01034952 0.98715531 0.021051   0.021051\n",
      " 0.021051   0.56763079 0.00561726 0.01034952 0.28066229 0.0093701\n",
      " 0.62626453 0.20502118 0.42708271 0.99081334 0.98715531 0.021051\n",
      " 1.12303111 0.86012146 0.01034952 0.16178664 0.98715531 0.74236334\n",
      " 0.48088522]\n",
      "[1.01091359 0.0374499  0.01415342 0.98621737 0.02011306 0.02011306\n",
      " 0.02011306 0.56669285 0.00467932 0.01415342 0.27972435 0.01317401\n",
      " 0.63006843 0.19979309 0.43088661 0.9898754  0.98621737 0.02011306\n",
      " 1.12209317 0.85918352 0.01415342 0.15655856 0.98621737 0.7414254\n",
      " 0.48468913]\n",
      "[1.010855   0.03739131 0.01277359 0.98615878 0.02005446 0.02005446\n",
      " 0.02005446 0.56663426 0.00462072 0.01277359 0.27966576 0.01179418\n",
      " 0.65390499 0.19580006 0.43082802 0.98981681 0.98615878 0.02005446\n",
      " 1.12203458 0.85912493 0.01277359 0.15256552 0.98615878 0.74136681\n",
      " 0.50852569]\n",
      "[1.01029514 0.03683145 0.01153175 0.98559892 0.0194946  0.0194946\n",
      " 0.0194946  0.5660744  0.00406086 0.01153175 0.2791059  0.01055233\n",
      " 0.66762722 0.19455821 0.44455024 0.98925695 0.98559892 0.0194946\n",
      " 1.12147472 0.85856507 0.01153175 0.14897179 0.98559892 0.74080695\n",
      " 0.52224791]\n",
      "[1.01066794 0.03555174 0.01025204 0.98597172 0.01821489 0.01821489\n",
      " 0.01821489 0.5664472  0.00443366 0.01025204 0.27782619 0.00570659\n",
      " 0.66634751 0.1932785  0.44492304 0.98962975 0.98597172 0.01821489\n",
      " 1.14424584 0.85893787 0.01025204 0.14934459 0.98597172 0.74117975\n",
      " 0.52262071]\n",
      "[ 1.01409357  0.03401575  0.00871605  0.98939735  0.0166789   0.0166789\n",
      "  0.0166789   0.55487412 -0.00713941  0.00871605  0.26625311  0.0041706\n",
      "  0.66481152  0.19670414  0.44834868  0.99133655  0.98939735  0.0166789\n",
      "  1.14595265  0.86064468  0.00871605  0.15277023  0.98939735  0.72960667\n",
      "  0.52604635]\n",
      "[ 1.01712776  0.03704994  0.01175024  0.98866018  0.01594173  0.01594173\n",
      "  0.01594173  0.55413694 -0.00787659  0.01175024  0.26551593  0.0072048\n",
      "  0.66784571  0.1923777   0.45138287  0.99059938  0.98866018  0.01594173\n",
      "  1.14521547  0.8599075   0.01175024  0.14844379  0.98866018  0.72886949\n",
      "  0.52908054]\n",
      "[ 1.01671551  0.03663769  0.01034058  0.98824793  0.01552948  0.01552948\n",
      "  0.01552948  0.55372469 -0.00828884  0.01034058  0.26510368  0.00579514\n",
      "  0.67729898  0.19096804  0.46083614  0.99018713  0.98824793  0.01552948\n",
      "  1.14480322  0.85949525  0.01034058  0.14445391  0.98824793  0.72845724\n",
      "  0.53853381]\n",
      "[ 1.01708356  0.03549533  0.00919822  0.98861597  0.01438711  0.01438711\n",
      "  0.01438711  0.55409274 -0.0079208   0.00919822  0.26396131  0.00465277\n",
      "  0.67615661  0.18982567  0.46120419  0.99055517  0.98861597  0.01438711\n",
      "  1.16370592  0.8598633   0.00919822  0.14482196  0.98861597  0.72882529\n",
      "  0.53890186]\n",
      "[ 1.01711312  0.03552488  0.00922777  0.98864553  0.01441667  0.01441667\n",
      "  0.01441667  0.51689097 -0.00789124  0.00922777  0.26399087  0.00468233\n",
      "  0.67618617  0.18985523  0.46123375  0.99186256  0.98864553  0.01441667\n",
      "  1.16846446  0.86117069  0.00922777  0.14485152  0.98864553  0.72885485\n",
      "  0.53893141]\n",
      "[ 1.02012473  0.03479077  0.00849366  0.99165714  0.01368255  0.01368255\n",
      "  0.01368255  0.51587054 -0.00891167  0.00849366  0.25777553  0.00394822\n",
      "  0.67545206  0.18912112  0.46424536  0.99084213  0.99165714  0.01368255\n",
      "  1.16224913  0.86015025  0.00849366  0.14786313  0.99165714  0.72783441\n",
      "  0.54194303]\n",
      "[ 1.02265813  0.03732416  0.01102706  0.99106798  0.01309339  0.01309339\n",
      "  0.01309339  0.51528137 -0.00950084  0.01102706  0.25718637  0.00648161\n",
      "  0.67798545  0.18528123  0.46677876  0.99025296  0.99106798  0.01309339\n",
      "  1.16165996  0.85956108  0.01102706  0.14851453  0.99106798  0.72724524\n",
      "  0.54447642]\n",
      "[ 1.02288289  0.03754893  0.01125182  0.99129274  0.01331815  0.01331815\n",
      "  0.01331815  0.49147039 -0.03331182  0.01125182  0.25741113  0.00670638\n",
      "  0.67821022  0.18550599  0.46700352  0.99122766  0.99129274  0.01331815\n",
      "  1.16473688  0.862638    0.01125182  0.14873929  0.99129274  0.72747001\n",
      "  0.54470119]\n",
      "[ 1.02291374  0.03757978  0.00999266  0.99132359  0.013349    0.013349\n",
      "  0.013349    0.49150124 -0.03328097  0.00999266  0.25744198  0.00544721\n",
      "  0.69418621  0.18424683  0.46703437  0.99125851  0.99132359  0.013349\n",
      "  1.16476772  0.86266885  0.00999266  0.14505671  0.99132359  0.72750086\n",
      "  0.56067718]\n",
      "[ 1.02564016  0.03680542  0.0092183   0.99405001  0.01257464  0.01257464\n",
      "  0.01257464  0.49069504 -0.03408717  0.0092183   0.25178646  0.00467286\n",
      "  0.69341185  0.18347248  0.46976078  0.99045231  0.99405001  0.01257464\n",
      "  1.1591122   0.86186265  0.0092183   0.14778313  0.99405001  0.72669466\n",
      "  0.5634036 ]\n",
      "[ 1.0255657   0.03673096  0.00914384  0.99397555  0.01250019  0.01250019\n",
      "  0.01250019  0.45983304 -0.03416163  0.00914384  0.251712    0.0045984\n",
      "  0.6933374   0.18339802  0.46968633  0.99171743  0.99397555  0.01250019\n",
      "  1.16382791  0.86312777  0.00914384  0.14770867  0.99397555  0.7266202\n",
      "  0.56332914]\n",
      "[ 1.02637982  0.03754509  0.0074955   0.99478967  0.01331431  0.01331431\n",
      "  0.01331431  0.46064717 -0.0333475   0.0074955   0.25252612  0.0038749\n",
      "  0.69415152  0.18174968  0.47050045  0.99253155  0.99478967  0.01331431\n",
      "  1.16464203  0.86394189  0.0074955   0.14606033  0.99478967  0.72743432\n",
      "  0.56414326]\n",
      "[ 1.02806962  0.03923489  0.00918531  0.99421275  0.01273739  0.01273739\n",
      "  0.01273739  0.46007025 -0.03392442  0.00918531  0.25194921  0.00322375\n",
      "  0.69584132  0.18343948  0.47219025  0.99195464  0.99421275  0.01273739\n",
      "  1.16406511  0.86336497  0.00918531  0.14775014  0.99421275  0.72685741\n",
      "  0.56583307]\n",
      "[ 1.02784958  0.03901485  0.00820357  0.99399271  0.01251735  0.01251735\n",
      "  0.01251735  0.4598502  -0.03414447  0.00820357  0.25172916  0.00224202\n",
      "  0.70628459  0.18245775  0.48263352  0.99173459  0.99399271  0.01251735\n",
      "  1.16384507  0.86314493  0.00820357  0.14443283  0.99399271  0.72663736\n",
      "  0.57627633]\n",
      "[ 1.02801413  0.03917939  0.00836812  0.99415726  0.01268189  0.01268189\n",
      "  0.01268189  0.44004297 -0.05395171  0.00836812  0.25189371  0.00240656\n",
      "  0.70644914  0.18262229  0.48279807  0.99287148  0.99415726  0.01268189\n",
      "  1.16808749  0.86428182  0.00836812  0.14459738  0.99415726  0.72680191\n",
      "  0.57644088]\n",
      "[ 1.03038006  0.03853899  0.00772771  0.99652319  0.01204149  0.01204149\n",
      "  0.01204149  0.43936916 -0.05462551  0.00772771  0.24649741  0.00176616\n",
      "  0.70580873  0.18198189  0.485164    0.99219768  0.99652319  0.01204149\n",
      "  1.1626912   0.86360802  0.00772771  0.14696331  0.99652319  0.7261281\n",
      "  0.57880681]\n",
      "[ 1.03029687  0.0384558   0.00764453  0.99644     0.0119583   0.0119583\n",
      "  0.0119583   0.41363975 -0.05470869  0.00764453  0.24641423  0.00168297\n",
      "  0.70572555  0.1818987   0.48508081  0.99328826  0.99644     0.0119583\n",
      "  1.16681288  0.8646986   0.00764453  0.14688013  0.99644     0.72604492\n",
      "  0.57872362]\n",
      "[ 1.03098394e+00  3.91428680e-02  6.12748831e-03  9.97127066e-01\n",
      "  1.26453686e-02  1.26453686e-02  1.26453686e-02  4.14326814e-01\n",
      " -5.40216305e-02  6.12748831e-03  2.47101293e-01  1.65931803e-04\n",
      "  7.06412611e-01  1.80381663e-01  4.85767877e-01  9.93975325e-01\n",
      "  9.97127066e-01  1.26453686e-02  1.16749994e+00  8.65385661e-01\n",
      "  6.12748831e-03  1.45363084e-01  9.97127066e-01  7.26731983e-01\n",
      "  5.79410689e-01]\n",
      "[ 1.03285033  0.04100926  0.00799388  0.99659232  0.01211063  0.01211063\n",
      "  0.01211063  0.41379207 -0.05455637  0.00799388  0.24656655  0.00203232\n",
      "  0.708279    0.18224805  0.48763427  0.99344058  0.99659232  0.01211063\n",
      "  1.1669652   0.86485092  0.00799388  0.14626449  0.99659232  0.72619724\n",
      "  0.58127708]\n",
      "[ 1.03502863  0.04042479  0.00740941  0.99877062  0.01152616  0.01152616\n",
      "  0.01152616  0.4131706  -0.05517785  0.00740941  0.24156281  0.00144785\n",
      "  0.70769453  0.18166358  0.48981257  0.99281911  0.99877062  0.01152616\n",
      "  1.16196146  0.86422945  0.00740941  0.14844279  0.99877062  0.72557577\n",
      "  0.58345538]\n",
      "[ 1.0351134   0.04050956  0.00749418  0.9988554   0.01161093  0.01161093\n",
      "  0.01161093  0.39674901 -0.07159944  0.00749418  0.24164758  0.00153262\n",
      "  0.7077793   0.18174835  0.48989734  0.99384755  0.9988554   0.01161093\n",
      "  1.165937    0.86525788  0.00749418  0.14852756  0.9988554   0.72566054\n",
      "  0.58354015]\n",
      "[ 1.03491249e+00  4.03086492e-02  6.60635857e-03  9.98654486e-01\n",
      "  1.14100175e-02  1.14100175e-02  1.14100175e-02  3.96548098e-01\n",
      " -7.18003464e-02  6.60635857e-03  2.41446671e-01  6.44802055e-04\n",
      "  7.17008739e-01  1.80860533e-01  4.99126776e-01  9.93646638e-01\n",
      "  9.98654486e-01  1.14100175e-02  1.16573609e+00  8.65056974e-01\n",
      "  6.60635857e-03  1.45239278e-01  9.98654486e-01  7.25459629e-01\n",
      "  5.92769587e-01]\n",
      "[ 1.03552936e+00  4.09255193e-02  5.30709505e-03  9.99271356e-01\n",
      "  1.20268876e-02  1.20268876e-02  1.20268876e-02  3.97164969e-01\n",
      " -7.11834763e-02  5.30709505e-03  2.42063541e-01 -6.54461465e-04\n",
      "  7.17625609e-01  1.79561270e-01  4.99743646e-01  9.94263509e-01\n",
      "  9.99271356e-01  1.20268876e-02  1.16635296e+00  8.65673844e-01\n",
      "  5.30709505e-03  1.43940014e-01  9.99271356e-01  7.26076499e-01\n",
      "  5.93386457e-01]\n",
      "[ 1.03699892e+00  4.23950788e-02  6.77665458e-03  9.98777942e-01\n",
      "  1.15334736e-02  1.15334736e-02  1.15334736e-02  3.96671555e-01\n",
      " -7.16768903e-02  6.77665458e-03  2.41570127e-01 -1.04744520e-03\n",
      "  7.19095169e-01  1.81030829e-01  5.01213205e-01  9.93770095e-01\n",
      "  9.98777942e-01  1.15334736e-02  1.16585955e+00  8.65180430e-01\n",
      "  6.77665458e-03  1.45409574e-01  9.98777942e-01  7.25583085e-01\n",
      "  5.94856017e-01]\n",
      "[ 1.03731279  0.04198134  0.00636291  0.99909181  0.01111973  0.01111973\n",
      "  0.01111973  0.39698543 -0.07649055  0.00636291  0.24115638 -0.00146119\n",
      "  0.71868143  0.18061709  0.49639955  0.99408397  0.99909181  0.01111973\n",
      "  1.16544581  0.86036677  0.00636291  0.14572345  0.99909181  0.72589696\n",
      "  0.59444227]\n",
      "[ 1.03854688e+00  4.32154268e-02  7.59700254e-03  9.98632502e-01\n",
      "  1.06604192e-02  1.06604192e-02  1.06604192e-02  3.96526115e-01\n",
      " -7.69498612e-02  7.59700254e-03  2.40697073e-01 -2.27097239e-04\n",
      "  7.19915517e-01  1.81851177e-01  4.97633637e-01  9.93624654e-01\n",
      "  9.98632502e-01  1.06604192e-02  1.16498649e+00  8.59907460e-01\n",
      "  7.59700254e-03  1.46957536e-01  9.98632502e-01  7.25437645e-01\n",
      "  5.95676365e-01]\n",
      "[ 1.03837003e+00  4.30385739e-02  6.74365996e-03  9.98455649e-01\n",
      "  1.04835663e-02  1.04835663e-02  1.04835663e-02  3.96349262e-01\n",
      " -7.71267141e-02  6.74365996e-03  2.40520220e-01 -1.08043983e-03\n",
      "  7.26641875e-01  1.80997834e-01  5.04359995e-01  9.93447802e-01\n",
      "  9.98455649e-01  1.04835663e-02  1.16480964e+00  8.59730607e-01\n",
      "  6.74365996e-03  1.44289139e-01  9.98455649e-01  7.25260792e-01\n",
      "  6.02402723e-01]\n",
      "[ 1.03893843  0.04360697  0.00552189  0.99902405  0.01105197  0.01105197\n",
      "  0.01105197  0.39691766 -0.07655831  0.00552189  0.24108862 -0.00230221\n",
      "  0.72721027  0.17977606  0.50492839  0.9940162   0.99902405  0.01105197\n",
      "  1.16537804  0.86029901  0.00552189  0.14306737  0.99902405  0.72582919\n",
      "  0.60297112]\n",
      "[ 1.03887685  0.0435454   0.00546031  0.99896247  0.01099039  0.01099039\n",
      "  0.01099039  0.3754334  -0.07661989  0.00546031  0.24102705 -0.00236379\n",
      "  0.7271487   0.17971449  0.50486682  0.99488712  0.99896247  0.01099039\n",
      "  1.16901192  0.86116992  0.00546031  0.14300579  0.99896247  0.72576762\n",
      "  0.60290955]\n",
      "[ 1.04008560e+00  4.47541466e-02  6.66906128e-03  9.98506477e-01\n",
      "  1.05343940e-02  1.05343940e-02  1.05343940e-02  3.74977402e-01\n",
      " -7.70758865e-02  6.66906128e-03  2.40571048e-01 -1.15503850e-03\n",
      "  7.28357448e-01  1.80923236e-01  5.06075568e-01  9.94431117e-01\n",
      "  9.98506477e-01  1.05343940e-02  1.16855593e+00  8.60713922e-01\n",
      "  6.66906128e-03  1.44214540e-01  9.98506477e-01  7.25311619e-01\n",
      "  6.04118296e-01]\n",
      "[ 1.04446208  0.04434924  0.00626416  0.99851099  0.01012949  0.01012949\n",
      "  0.01012949  0.37498192 -0.08128808  0.00626416  0.24016614 -0.00155994\n",
      "  0.72795254  0.18051833  0.50186338  0.99443563  0.99851099  0.01012949\n",
      "  1.16815102  0.85650173  0.00626416  0.14859102  0.99851099  0.72531613\n",
      "  0.60371339]\n",
      "[ 1.04487941e+00  4.47665669e-02  6.68148158e-03  9.98928318e-01\n",
      "  1.05468143e-02  1.05468143e-02  1.05468143e-02  3.73428099e-01\n",
      " -8.28418935e-02  6.68148158e-03  2.38612323e-01 -1.14261821e-03\n",
      "  7.28369868e-01  1.80935656e-01  5.02280706e-01  9.92881814e-01\n",
      "  9.98928318e-01  1.05468143e-02  1.17103689e+00  8.59387607e-01\n",
      "  6.68148158e-03  1.49008345e-01  9.98928318e-01  7.23762316e-01\n",
      "  6.04130716e-01]\n",
      "[ 1.04478267  0.04466983  0.00658475  0.99883158  0.01045008  0.01045008\n",
      "  0.01045008  0.3542928  -0.08293863  0.00658475  0.23851559 -0.00123935\n",
      "  0.72827313  0.18083892  0.50218397  0.99375722  0.99883158  0.01045008\n",
      "  1.1743656   0.86026302  0.00658475  0.14891161  0.99883158  0.72366558\n",
      "  0.60403398]\n",
      "[ 1.04460594  0.04449311  0.00577837  0.99865486  0.01027335  0.01027335\n",
      "  0.01027335  0.35411607 -0.08311535  0.00577837  0.23833886 -0.00204573\n",
      "  0.73623536  0.18003254  0.51014619  0.9935805   0.99865486  0.01027335\n",
      "  1.17418887  0.86008629  0.00577837  0.14627073  0.99865486  0.72348886\n",
      "  0.6119962 ]\n",
      "[ 1.04497524  0.0440512   0.00533646  0.99902416  0.00983145  0.00983145\n",
      "  0.00983145  0.35448537 -0.08274605  0.00533646  0.23381665 -0.00248764\n",
      "  0.73579345  0.17959064  0.51051549  0.9939498   0.99902416  0.00983145\n",
      "  1.16966666  0.86045559  0.00533646  0.14664003  0.99902416  0.72385816\n",
      "  0.6123655 ]\n",
      "[ 1.04547404  0.04455     0.00426282  0.99952295  0.01033024  0.01033024\n",
      "  0.01033024  0.35498416 -0.08224726  0.00426282  0.23431544 -0.00356128\n",
      "  0.73629225  0.178517    0.51101428  0.99444859  0.99952295  0.01033024\n",
      "  1.17016545  0.86095438  0.00426282  0.14556639  0.99952295  0.72435695\n",
      "  0.61286429]\n",
      "[ 1.04667399  0.04574995  0.00546277  0.99911155  0.00991884  0.00991884\n",
      "  0.00991884  0.35457276 -0.08265866  0.00546277  0.23390404 -0.00413162\n",
      "  0.7374922   0.17971695  0.51221424  0.99403719  0.99911155  0.00991884\n",
      "  1.16975405  0.86054298  0.00546277  0.14676634  0.99911155  0.72394555\n",
      "  0.61406425]\n",
      "[ 1.04677406  0.04548029  0.00519312  0.99921162  0.00964919  0.00964919\n",
      "  0.00964919  0.35798625 -0.08292832  0.00519312  0.23363438 -0.00440127\n",
      "  0.73722254  0.17944729  0.51194458  0.99413726  0.99921162  0.00964919\n",
      "  1.16948439  0.86027332  0.00519312  0.15017983  0.99921162  0.72404561\n",
      "  0.61379459]\n",
      "[ 1.04703697  0.04514685  0.00485967  0.99947453  0.00931574  0.00931574\n",
      "  0.00931574  0.35824916 -0.08652499  0.00485967  0.23330094 -0.00473472\n",
      "  0.7368891   0.17911385  0.50834791  0.99440017  0.99947453  0.00931574\n",
      "  1.16915095  0.85667665  0.00485967  0.15044274  0.99947453  0.72430853\n",
      "  0.61346115]\n",
      "[ 1.04804713  0.046157    0.00586982  0.9990879   0.00892911  0.00892911\n",
      "  0.00892911  0.35786253 -0.08691163  0.00586982  0.2329143  -0.00372457\n",
      "  0.73789925  0.180124    0.50935806  0.99401354  0.9990879   0.00892911\n",
      "  1.16876431  0.85629002  0.00586982  0.1514529   0.9990879   0.7239219\n",
      "  0.6144713 ]\n",
      "[ 1.04786154  0.04597141  0.0052087   0.99890231  0.00874351  0.00874351\n",
      "  0.00874351  0.35767694 -0.08709722  0.0052087   0.23272871 -0.0043857\n",
      "  0.74520834  0.17946287  0.51666716  0.99382795  0.99890231  0.00874351\n",
      "  1.16857872  0.85610443  0.0052087   0.14938695  0.99890231  0.7237363\n",
      "  0.62178039]\n",
      "[ 1.04794934  0.04605921  0.0052965   0.99899011  0.00883132  0.00883132\n",
      "  0.00883132  0.34543715 -0.099337    0.0052965   0.23281651 -0.00429789\n",
      "  0.74529615  0.17955067  0.51675496  0.99453538  0.99899011  0.00883132\n",
      "  1.17150525  0.85681186  0.0052965   0.14947475  0.99899011  0.72382411\n",
      "  0.6218682 ]\n",
      "[ 1.05132352  0.04467694  0.00545696  0.9990397   0.00744905  0.00744905\n",
      "  0.00744905  0.34548674 -0.10071927  0.00545696  0.23143425 -0.00413744\n",
      "  0.74545661  0.17971113  0.51537269  0.99458497  0.9990397   0.00744905\n",
      "  1.17012299  0.85542959  0.00545696  0.15284893  0.9990397   0.7238737\n",
      "  0.62202866]\n",
      "[ 1.05118304  0.04453646  0.00483711  0.99889922  0.00730857  0.00730857\n",
      "  0.00730857  0.34534626 -0.10085975  0.00483711  0.23129377 -0.00475728\n",
      "  0.75196819  0.17909129  0.52188428  0.99444449  0.99889922  0.00730857\n",
      "  1.16998251  0.85528911  0.00483711  0.1508362   0.99889922  0.72373322\n",
      "  0.62854024]\n",
      "[ 1.05161464  0.04496807  0.00391695  0.99933083  0.00774017  0.00774017\n",
      "  0.00774017  0.34577787 -0.10042815  0.00391695  0.23172537 -0.00567744\n",
      "  0.75239979  0.17817113  0.52231588  0.9948761   0.99933083  0.00774017\n",
      "  1.17041411  0.85572072  0.00391695  0.14991604  0.99933083  0.72416482\n",
      "  0.62897184]\n",
      "[ 1.05193089  0.04528431  0.00423319  0.99964707  0.00805641  0.00805641\n",
      "  0.00805641  0.344777   -0.10142901  0.00423319  0.2307245  -0.0053612\n",
      "  0.75271603  0.17848737  0.52263212  0.99538849  0.99964707  0.00805641\n",
      "  1.16941324  0.85471985  0.00423319  0.15023228  0.99964707  0.72735478\n",
      "  0.62928808]\n",
      "[ 1.05186376  0.04521718  0.00416607  0.99957994  0.00798929  0.00798929\n",
      "  0.00798929  0.32883815 -0.10149614  0.00416607  0.23065738 -0.00542832\n",
      "  0.75264891  0.17842024  0.522565    0.99607565  0.99957994  0.00798929\n",
      "  1.17225632  0.85540701  0.00416607  0.15016516  0.99957994  0.72728765\n",
      "  0.62922096]\n",
      "[ 1.05226126  0.04561468  0.00331301  0.99997744  0.00838679  0.00838679\n",
      "  0.00838679  0.32923565 -0.10109864  0.00331301  0.23105488 -0.00628138\n",
      "  0.75304641  0.17756719  0.5229625   0.99647315  0.99997744  0.00838679\n",
      "  1.17265382  0.85580452  0.00331301  0.1493121   0.99997744  0.72768515\n",
      "  0.62961846]\n",
      "[ 1.05233594  0.04580332  0.00350165  1.00005212  0.00857543  0.00857543\n",
      "  0.00857543  0.33199229 -0.10383945  0.00350165  0.22831406 -0.00609274\n",
      "  0.75323505  0.17775582  0.52315113  0.99654783  1.00005212  0.00857543\n",
      "  1.16991301  0.8530637   0.00350165  0.15206875  1.00005212  0.72775983\n",
      "  0.62980709]\n",
      "[ 1.05325746  0.04672484  0.00442317  0.999742    0.0082653   0.0082653\n",
      "  0.0082653   0.33168217 -0.10414958  0.00442317  0.22800394 -0.00638061\n",
      "  0.75415656  0.17867734  0.52407265  0.99623771  0.999742    0.0082653\n",
      "  1.16960288  0.85275357  0.00442317  0.15299026  0.999742    0.72744971\n",
      "  0.63072861]\n",
      "[ 1.05319803  0.04666541  0.00436373  0.99968256  0.00820587  0.00820587\n",
      "  0.00820587  0.31732099 -0.10420901  0.00436373  0.2279445  -0.00644004\n",
      "  0.75409713  0.17861791  0.52401322  0.99682787  0.99968256  0.00820587\n",
      "  1.17242699  0.85334374  0.00436373  0.15293083  0.99968256  0.72739028\n",
      "  0.63066918]\n",
      "[ 1.05442089  0.04788827  0.0055866   0.99940011  0.00792341  0.00792341\n",
      "  0.00792341  0.31703854 -0.10449147  0.0055866   0.22766205 -0.00521718\n",
      "  0.75532     0.17667733  0.52523608  0.99654542  0.99940011  0.00792341\n",
      "  1.17214454  0.85306128  0.0055866   0.15099026  0.99940011  0.72710782\n",
      "  0.63189205]\n",
      "[ 1.05466962  0.04760704  0.00530536  0.99964884  0.00764217  0.00764217\n",
      "  0.00764217  0.31728727 -0.10424274  0.00530536  0.22391976 -0.00549841\n",
      "  0.75503876  0.1763961   0.52548481  0.99679415  0.99964884  0.00764217\n",
      "  1.16840225  0.85331001  0.00530536  0.15123899  0.99964884  0.72735655\n",
      "  0.63923812]\n",
      "[ 1.05451815  0.04745556  0.00471879  0.99949736  0.00749069  0.00749069\n",
      "  0.00749069  0.31713579 -0.10439422  0.00471879  0.22376829 -0.00608499\n",
      "  0.76102317  0.17580952  0.53146922  0.99664267  0.99949736  0.00749069\n",
      "  1.16825077  0.85315853  0.00471879  0.14942348  0.99949736  0.72720507\n",
      "  0.64522253]\n",
      "[ 1.05488907  0.04782648  0.00393179  0.99986828  0.00786162  0.00786162\n",
      "  0.00786162  0.31750671 -0.10402329  0.00393179  0.22413921 -0.00687199\n",
      "  0.76139409  0.17502252  0.53184014  0.99701359  0.99986828  0.00786162\n",
      "  1.1686217   0.85352946  0.00393179  0.14863648  0.99986828  0.72757599\n",
      "  0.64559345]\n",
      "[ 1.05519572  0.04813313  0.00423844  1.00017493  0.00816827  0.00816827\n",
      "  0.00816827  0.31669575 -0.10483425  0.00423844  0.22332825 -0.00656533\n",
      "  0.76170074  0.17532917  0.53214679  0.99620263  1.00017493  0.00816827\n",
      "  1.16781074  0.8527185   0.00423844  0.14894313  1.00017493  0.73395772\n",
      "  0.6459001 ]\n",
      "[ 1.05540562  0.04834303  0.00444834  1.00038483  0.00837817  0.00837817\n",
      "  0.00837817  0.30998715 -0.11154286  0.00444834  0.22353815 -0.00635544\n",
      "  0.76191064  0.17553907  0.53235669  0.99707512  1.00038483  0.00837817\n",
      "  1.16868322  0.85359098  0.00444834  0.14915303  1.00038483  0.72724912\n",
      "  0.64611   ]\n",
      "[ 1.05575824  0.04869565  0.00368838  1.00073745  0.00873078  0.00873078\n",
      "  0.00873078  0.31033976 -0.11119024  0.00368838  0.22389076 -0.00711539\n",
      "  0.76226326  0.17477912  0.53270931  0.99742773  1.00073745  0.00873078\n",
      "  1.16903584  0.8539436   0.00368838  0.14839307  1.00073745  0.72760173\n",
      "  0.64646262]\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape)\n",
    "cnt = 0\n",
    "for y_pred in gbrt.staged_predict(X_val):\n",
    "    print(y_pred)\n",
    "    cnt+=1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best estimators number :  120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=119,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_n_estimators = np.argmin(errors)\n",
    "print('best estimators number : ',bst_n_estimators+1)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/07_10.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The validation errors are represented on the left of Figure 7-11, and the best model’s predictions are\n",
    "represented on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is also possible to implement early stopping by actually stopping training early (instead of training a large number of trees first and then looking back to find the optimal number). \n",
    "* You can do so by setting warm_start=True, which makes Scikit-Learn **keep existing trees when the fit() method is called**, allowing incremental training. \n",
    "* The following code stops training when the validation error does not improve for five iterations in a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "\n",
    "min_val_error = float(\"inf\") #infinite number\n",
    "error_going_up = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    \n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The GradientBoostingRegressor class also supports a *subsample* hyperparameter, which **specifies the fraction of training instances to be used for training each tree**. \n",
    "* For example, if subsample=0.25, then each tree is trained on 25% of the training instances, selected randomly. \n",
    "* As you can probably guess by now, this trades **a higher bias for a lower variance.** \n",
    "* It also speeds up training considerably. \n",
    "* This technique is called **Stochastic Gradient Boosting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">NOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to use Gradient Boosting with other cost functions. This is controlled by the loss hyperparameter (see Scikit-\n",
    "Learn’s documentation for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The last Ensemble method we will discuss in this chapter is called **stacking** (short for stacked generalization).\n",
    "* It is based on a simple idea: \n",
    "    * instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, **why don’t we train a model to perform this aggregation?** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/07_12.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Figure 7-12 shows such an ensemble performing a regression task on a new instance. \n",
    "    * Each of the bottom three predictors predicts a different value (3.1, 2.7, and 2.9), and **then the final predictor (called a *blender*, or a meta learner) takes these predictions as inputs and makes the final prediction (3.0).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To train the blender, a common approach is to use a **hold-out set.** Let’s see how it works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/07_13.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, the training set is split in two subsets. The first subset is used to train the predictors in the first layer (see Figure 7-13)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/07_14.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, the first layer predictors are used to make predictions on the second (held-out) set (see Figure 7-14). \n",
    "    * This ensures that **the predictions are “clean,” since the predictors never saw these instances during training.** \n",
    "* Now for each instance in the hold-out set there are three predicted values.\n",
    "* We can create **a new training set using these predicted values as input features** (which makes this new training set **three-dimensional**), and keeping the target values. \n",
    "* The blender is trained on this new training set, so it learns to predict the target value given the first layer’s predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is actually possible to train several different blenders this way (e.g., one using Linear Regression, another using Random Forest Regression, and so on): we get a whole layer of blenders. \n",
    "* The trick is to split the training set into three subsets: \n",
    "    * the first one is used to train the first layer\n",
    "    * the second one is used to create the training set used to train the second layer (using predictions made by the predictors of the first layer)\n",
    "    * the third one is used to create the training set to train the third layer(using predictions made by the predictors of the second layer). \n",
    "* Once this is done, we can make a prediction for a new instance by going through each layer sequentially, as shown in Figure 7-15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6-2](./img/07_15.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unfortunately, **Scikit-Learn does not support stacking directly**, but it is not too hard to roll out your own implementation .\n",
    "* Alternatively, you can use an open source implementation such as brew (available at https://github.com/viisar/brew)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
